#!/usr/bin/env python

import tkinter as tk
from tkinter import scrolledtext
from tkinter import messagebox
from selenium import webdriver  # Import Selenium
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
import regex  # Import regex library for advanced pattern matching
from urllib.parse import urljoin
import time  # Import the time module

# Maximum recursion depth for fetching data from linked pages
DEFAULT_MAX_DEPTH = 5

def fetch_data(url, max_depth, visited_urls=set(), depth=0):
    try:
        # Limit recursion depth to avoid opening too many files
        if depth > max_depth:
            return

        # Add the URL to the set of visited URLs
        visited_urls.add(url)

        # Provide user feedback
        text_area.insert(tk.END, f"Fetching data from {url}...\n")

        # Use Selenium to get dynamic content
        service = Service('path_to_chromedriver')  # Update with your ChromeDriver path
        service.start()
        driver = webdriver.Remote(service.service_url)
        driver.get(url)
        time.sleep(2)  # Allow time for page to load (adjust as needed)

        # Extract HTML content after page fully loads
        html_content = driver.page_source
        driver.quit()  # Quit the Selenium WebDriver

        soup = BeautifulSoup(html_content, 'html.parser')
        text = soup.get_text(separator='\n', strip=True)  # Extract text from HTML content
        cleaned_text = clean_text(text)  # Clean and format the text
        extracted_data = extract_data(cleaned_text)  # Call function from second script
        display_data(extracted_data)  # Display the extracted data in the text area

        # Find all anchor tags (links) on the page
        links = soup.find_all('a', href=True)
        for link in links:
            absolute_url = urljoin(url, link['href'])
            if absolute_url not in visited_urls:
                fetch_data(absolute_url, max_depth, visited_urls, depth + 1)  # Recursively fetch data from the links

        # Add a wait time of 4 seconds between each page scrape
        time.sleep(2)

    except Exception as e:
        messagebox.showerror("Error", f"An error occurred: {str(e)}")

def clean_text(text):
    # Remove unwanted characters and extra whitespaces
    cleaned_text = ' '.join(text.split())  # Remove extra whitespaces
    return cleaned_text

def extract_data(text):
    # Call functions to extract data based on checkbox states
    ssns = extract_ssns(text) if ssn_check_var.get() else []
    emails = extract_emails(text) if email_check_var.get() else []
    addresses = extract_addresses(text) if address_check_var.get() else []
    phones = extract_phones(text) if phone_check_var.get() else []
    names = extract_names(text) if name_check_var.get() else []
    tables = extract_tables(text) if table_check_var.get() else []
    # You can further process the extracted data as needed
    extracted_data = f"SSNs: {ssns}\n\nEmails: {emails}\n\nAddresses: {addresses}\n\nPhone Numbers: {phones}\n\nPossible Names: {names}\n\nTables: {tables}"
    return extracted_data

def extract_ssns(text):
    # Use regex pattern to find SSNs in the text
    pattern = r'\b\d{3}-\d{2}-\d{4}\b'
    ssns = regex.findall(pattern, text)
    return ssns

def extract_emails(text):
    # Use regex pattern to find email addresses in the text
    pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    emails = regex.findall(pattern, text)
    return emails

def extract_addresses(text):
    # Define a regex pattern to match addresses
    pattern = r'\b\d+\s+\w+\s+\w+\b'
    addresses = regex.findall(pattern, text)
    return addresses

def extract_phones(text):
    # Define regex patterns to match phone numbers
    pattern1 = r'\+\d{1,2}\s?\(\d{3}\)\s?\d{3}-\d{4}'
    pattern2 = r'\(\d{3}\)\s?\d{3}-\d{4}'
    pattern3 = r'\+\d{1,2}\s?\d{3}-\d{3}-\d{4}'
    pattern4 = r'\d{3}-\d{3}-\d{4}'
    phones = regex.findall(f'({pattern1}|{pattern2}|{pattern3}|{pattern4})', text)
    return phones

def extract_names(text):
    # Define a regex pattern to match names
    pattern = r'\b[A-Z][a-z]+ [A-Z][a-z]+\b'
    names = regex.findall(pattern, text)
    return names

def extract_tables(text):
    # Parse HTML and find all table elements
    soup = BeautifulSoup(text, 'html.parser')
    tables = soup.find_all('table')

    table_data = []
    for table in tables:
        rows = table.find_all('tr')
        table_rows = []
        for row in rows:
            cols = row.find_all(['th', 'td'])
            row_data = [col.get_text(strip=True) for col in cols]
            table_rows.append(row_data)
        table_data.append(table_rows)

    return table_data

def display_data(data):
    text_area.delete('1.0', tk.END)
    text_area.insert(tk.END, data)

# Create the main window
root = tk.Tk()
root.title("UR-Scraped")

# Customize appearance of top choice area
top_choice_label = tk.Label(root, text="UR-Scraped", bg="red", fg="black", font=("Arial", 14, "bold"))
top_choice_label.pack(fill=tk.X)

# Create a frame for choice-related widgets with light blue background
choice_frame = tk.Frame(root, bg="#ADD8E6")  # Same shade of blue
choice_frame.pack(padx=10, pady=10)

# Create and place widgets inside the frame
depth_label = tk.Label(choice_frame, text="Depth Max 5:")
depth_label.grid(row=0, column=0, sticky="w")

depth_entry = tk.Entry(choice_frame, width=5)
depth_entry.grid(row=0, column=1, padx=(0, 10))
depth_entry.insert(0, DEFAULT_MAX_DEPTH)

url_label = tk.Label(choice_frame, text="Enter URL:")
url_label.grid(row=0, column=2, sticky="w")

url_entry = tk.Entry(choice_frame, width=50)
url_entry.grid(row=0, column=3)

ssn_check_var = tk.IntVar()  # Variable to store the SSN checkbox state
ssn_check = tk.Checkbutton(choice_frame, text="Extract SSNs", variable=ssn_check_var, bg="#ADD8E6")  # Light blue
ssn_check.grid(row=1, column=0, sticky="w")

email_check_var = tk.IntVar()  # Variable to store the email checkbox state
email_check = tk.Checkbutton(choice_frame, text="Extract Emails", variable=email_check_var, bg="#ADD8E6")  # Light blue
email_check.grid(row=1, column=1, sticky="w")

address_check_var = tk.IntVar()  # Variable to store the address checkbox state
address_check = tk.Checkbutton(choice_frame, text="Extract Addresses", variable=address_check_var, bg="#ADD8E6")  # Light blue
address_check.grid(row=1, column=2, sticky="w")

phone_check_var = tk.IntVar()  # Variable to store the phone checkbox state
phone_check = tk.Checkbutton(choice_frame, text="Extract Phone Numbers", variable=phone_check_var, bg="#ADD8E6")  # Light blue
phone_check.grid(row=2, column=0, sticky="w")

name_check_var = tk.IntVar()  # Variable to store the name checkbox state
name_check = tk.Checkbutton(choice_frame, text="Extract Possible Names", variable=name_check_var, bg="#ADD8E6")  # Light blue
name_check.grid(row=2, column=1, sticky="w")

table_check_var = tk.IntVar()  # Variable to store the table checkbox state
table_check = tk.Checkbutton(choice_frame, text="Extract Tables", variable=table_check_var, bg="#ADD8E6")  # Light blue
table_check.grid(row=2, column=2, sticky="w")

fetch_button = tk.Button(choice_frame, text="Fetch Data Press Twice", command=lambda: fetch_data(url_entry.get(), int(depth_entry.get())), bg="#ADD8E6")  # Light blue
fetch_button.grid(row=3, column=0, columnspan=3)

text_area = scrolledtext.ScrolledText(root, width=80, height=40)
text_area.pack()

# Run the main event loop
root.mainloop()
