#!/usr/bin/env python

import tkinter as tk
from tkinter import scrolledtext, messagebox, filedialog
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import regex
import logging
import time
import os
import threading

# Maximum recursion depth for fetching data from linked pages
DEFAULT_MAX_DEPTH = ""

# Set up logging to a file
logging.basicConfig(filename='scraping.log', level=logging.WARNING,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# Global variables
output_folder = ""
displayed_data = ""
stop_fetching = False

# Function to update console text
def update_console_text():
    try:
        # Read the contents of the log file
        with open('scraping.log', 'r') as log_file:
            log_contents = log_file.read()

        # Insert the log contents into the console_text widget
        console_text.delete('1.0', tk.END)
        console_text.insert(tk.END, log_contents)
        console_text.see(tk.END)  # Scroll to the bottom
    except Exception as e:
                logging.error(f"An error occurred while updating console text: {e}")

def fetch_data(url, max_depth, visited_urls=set(), depth=0):
    global displayed_data, stop_fetching
    try:
        # Limit recursion depth to avoid opening too many files
        if depth > max_depth or stop_fetching:
            return ''

        visited_urls = set()  # Initialize visited_urls as a set
        visited_urls.add(url)



        # Log message
        logging.debug(f"Fetching data from {url}...")

        # Use Selenium to get dynamic content
        chrome_driver_path = os.path.join(os.path.dirname(__file__), "chromedriver")
        service = Service(chrome_driver_path)
        service.start()
        driver = webdriver.Chrome(service=service)
        driver.get(url)
        # Add explicit wait logic here if needed

        # Extract HTML content after page fully loads
        time.sleep(2)  # Let's add a delay to ensure the page fully loads
        html_content = driver.page_source
        driver.quit()

        soup = BeautifulSoup(html_content, 'html.parser')
        text = soup.get_text(separator='\n', strip=True)  
        cleaned_text = clean_text(text)  
        extracted_data = extract_data(cleaned_text)
        displayed_data += f"\n\nData from {url}:\n{extracted_data}"
        display_data(displayed_data) 

        # Find all anchor tags (links) on the page
        links = soup.find_all('a', href=True)
        for link in links:
            absolute_url = urljoin(url, link['href'])
            if absolute_url not in visited_urls:
                fetch_data(absolute_url, max_depth, visited_urls, depth + 1)


        time.sleep(2)  
        return extracted_data

    except Exception as e:
        logging.error(f"An error occurred: {str(e)}")
        messagebox.showerror("Error", f"An error occurred: {str(e)}")
        return ''

def clean_text(text):
    cleaned_text = ' '.join(text.split()) 
    return cleaned_text

def extract_data(text):
    # Call functions to extract data based on checkbox states
    ssns = extract_ssns(text) if ssn_check_var.get() else []
    emails = extract_emails(text) if email_check_var.get() else []
    addresses = extract_addresses(text) if address_check_var.get() else []
    phones = extract_phones(text) if phone_check_var.get() else []
    names = extract_names(text) if name_check_var.get() else []
    tables = extract_tables(text) if table_check_var.get() else []
    extracted_data = f"SSNs: {ssns}\n\nEmails: {emails}\n\nAddresses: {addresses}\n\nPhone Numbers: {phones}\n\nPossible Names: {names}\n\nTables: {tables}"
    return extracted_data

def extract_ssns(text):
    # Use regex pattern to find SSNs in the text
    pattern = r'\b\d{3}-\d{2}-\d{4}\b'
    ssns = regex.findall(pattern, text)
    return ssns

def extract_emails(text):
    # Use regex pattern to find email addresses in the text
    pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'
    emails = regex.findall(pattern, text)
    return emails

def extract_addresses(text):
    # Define a regex pattern to match addresses
    pattern = r'\b\d+\s+\w+\s+\w+\b'
    addresses = regex.findall(pattern, text)
    return addresses

def extract_phones(text):
    # Define regex patterns to match phone numbers
    pattern1 = r'\+\d{1,2}\s?\(\d{3}\)\s?\d{3}-\d{4}'
    pattern2 = r'\(\d{3}\)\s?\d{3}-\d{4}'
    pattern3 = r'\+\d{1,2}\s?\d{3}-\d{3}-\d{4}'
    pattern4 = r'\d{3}-\d{3}-\d{4}'
    phones = regex.findall(f'({pattern1}|{pattern2}|{pattern3}|{pattern4})', text)
    return phones

def extract_names(text):
    # Define a regex pattern to match names
    pattern = r'\b[A-Z][a-z]+ [A-Z][a-z]+\b'
    names = regex.findall(pattern, text)
    return names

def extract_tables(text):
    # Parse HTML and find all table elements
    soup = BeautifulSoup(text, 'html.parser')
    tables = soup.find_all('table')

    table_data = []
    for table in tables:
        rows = table.find_all('tr')
        table_rows = []
        for row in rows:
            cols = row.find_all(['th', 'td'])
            row_data = [col.get_text(strip=True) for col in cols]
            table_rows.append(row_data)
        table_data.append(table_rows)

    return table_data

output_folder = ""

def browse_folder():
    try:
        output_folder = filedialog.askdirectory()
    except Exception as e:
        logging.error(f"An error occurred while browsing folder: {str(e)}")

def fetch_and_display_data():
    try:
        # Fetch data
        fetch_thread = threading.Thread(target=fetch_data_thread)
        fetch_thread.start()
    except Exception as e:
        logging.error(f"An error occurred while fetching and displaying data: {str(e)}")
        messagebox.showerror("Error", f"An error occurred: {str(e)}")

def fetch_data_thread():
    global displayed_data
    # Reset displayed_data and stop_fetching flag
    displayed_data = ""
    stop_fetching = False

    fetched_data = fetch_data(url_entry.get(), int(depth_entry.get()))
    if fetched_data is not None:
        # Display fetched data
        display_data(fetched_data)
    else:
        # Handle the case when fetch_data returns None
        messagebox.showerror("Error", "Failed to fetch data.")

def stop_and_display_data():
    global stop_fetching
    stop_fetching = True
    
def display_data(data):
    try:
        text_area.delete('1.0', tk.END)
        text_area.insert(tk.END, data)
    except Exception as e:
        logging.error(f"An error occurred while displaying data: {str(e)}")
        messagebox.showerror("Error", f"An error occurred: {str(e)}")

# Create the main window
root = tk.Tk()
root.title("UR-Scraped")

# Customize appearance of top choice area
top_choice_label = tk.Label(root, text="UR-Scraped", bg="red", fg="black", font=("Arial", 14, "bold"))
top_choice_label.pack(fill=tk.X)

# Create a frame for choice-related widgets with light blue background
choice_frame = tk.Frame(root, bg="#ADD8E6")  
choice_frame.pack(padx=10, pady=10)

# Create and place widgets inside the frame
depth_label = tk.Label(choice_frame, text="Depth Max 5:")
depth_label.grid(row=0, column=0, sticky="w")

depth_entry = tk.Entry(choice_frame, width=5)
depth_entry.grid(row=0, column=1, padx=(0, 10))
depth_entry.insert(0, "5")

url_label = tk.Label(choice_frame, text="Enter URL:")
url_label.grid(row=0, column=2, sticky="w")

url_entry = tk.Entry(choice_frame, width=50)
url_entry.grid(row=0, column=3)

ssn_check_var = tk.BooleanVar()  
email_check_var = tk.BooleanVar()  
address_check_var = tk.BooleanVar()  
phone_check_var = tk.BooleanVar()  
name_check_var = tk.BooleanVar()  
table_check_var = tk.BooleanVar()  

ssn_check = tk.Checkbutton(choice_frame, text="Extract SSNs", variable=ssn_check_var, bg="#ADD8E6")  
ssn_check.grid(row=1, column=0, sticky="w")

email_check = tk.Checkbutton(choice_frame, text="Extract Emails", variable=email_check_var, bg="#ADD8E6")  
email_check.grid(row=1, column=1, sticky="w")

address_check = tk.Checkbutton(choice_frame, text="Extract Addresses", variable=address_check_var, bg="#ADD8E6")  
address_check.grid(row=1, column=2, sticky="w")

phone_check = tk.Checkbutton(choice_frame, text="Extract Phone Numbers", variable=phone_check_var, bg="#ADD8E6")  
phone_check.grid(row=2, column=0, sticky="w")

name_check = tk.Checkbutton(choice_frame, text="Extract Possible Names", variable=name_check_var, bg="#ADD8E6")  
name_check.grid(row=2, column=1, sticky="w")

table_check = tk.Checkbutton(choice_frame, text="Extract Tables", variable=table_check_var, bg="#ADD8E6")  
table_check.grid(row=2, column=2, sticky="w")

browse_button = tk.Button(choice_frame, text="Browse Output Folder", command=browse_folder, bg="#ADD8E6")  
browse_button.grid(row=3, column=0, columnspan=3)

fetch_button = tk.Button(choice_frame, text="Fetch Data Press Twice", command=fetch_and_display_data, bg="#ADD8E6")
fetch_button.grid(row=4, column=0, columnspan=3)

stop_display_button = tk.Button(choice_frame, text="Stop and Display Data", command=stop_and_display_data, bg="red")
stop_display_button.grid(row=4, column=4, columnspan=3, pady=(10, 0))

text_area = scrolledtext.ScrolledText(root, width=80, height=20)
text_area.pack()

console_frame = tk.Frame(root, bg="#DDDDDD", padx=5, pady=5)
console_frame.pack(fill=tk.BOTH, expand=True)
console_label = tk.Label(console_frame, text="Console Logging:", bg="#DDDDDD")
console_label.pack(side=tk.TOP, anchor="w")
console_text = scrolledtext.ScrolledText(console_frame, bg="black", fg="white")
console_text.pack(fill=tk.BOTH, expand=True)

# Update console text initially
update_console_text()

# Run the GUI
root.mainloop()
